{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ML Metadata","text":"<p>ML Metadata (MLMD) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows. MLMD is an integral part of TensorFlow Extended (TFX), but is designed so that it can be used independently.</p> <p>Every run of a production ML pipeline generates metadata containing information about the various pipeline components, their executions (e.g. training runs), and resulting artifacts (e.g. trained models). In the event of unexpected pipeline behavior or errors, this metadata can be leveraged to analyze the lineage of pipeline components and debug issues. Think of this metadata as the equivalent of logging in software development.</p> <p>MLMD helps you understand and analyze all the interconnected parts of your ML pipeline instead of analyzing them in isolation and can help you answer questions about your ML pipeline such as:</p> <ul> <li>Which dataset did the model train on?</li> <li>What were the hyperparameters used to train the model?</li> <li>Which pipeline run created the model?</li> <li>Which training run led to this model?</li> <li>Which version of TensorFlow created this model?</li> <li>When was the failed model pushed?</li> </ul>"},{"location":"#metadata-store","title":"Metadata store","text":"<p>MLMD registers the following types of metadata in a database called the Metadata Store.</p> <ol> <li>Metadata about the artifacts generated through the components/steps of your     ML pipelines</li> <li>Metadata about the executions of these components/steps</li> <li>Metadata about pipelines and associated lineage information</li> </ol> <p>The Metadata Store provides APIs to record and retrieve metadata to and from the storage backend. The storage backend is pluggable and can be extended. MLMD provides reference implementations for SQLite (which supports in-memory and disk) and MySQL out of the box.</p> <p>This graphic shows a high-level overview of the various components that are part of MLMD.</p> <p></p>"},{"location":"#metadata-storage-backends-and-store-connection-configuration","title":"Metadata storage backends and store connection configuration","text":"<p>The <code>MetadataStore</code> object receives a connection configuration that corresponds to the storage backend used.</p> <ul> <li>Fake Database provides an in-memory DB (using SQLite) for fast     experimentation and local runs. The database is deleted when the store     object is destroyed.</li> </ul> <pre><code>from ml_metadata import metadata_store\nfrom ml_metadata.proto import metadata_store_pb2\n\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.fake_database.SetInParent() # Sets an empty fake database proto.\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>SQLite reads and writes files from disk.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nconnection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>MySQL connects to a MySQL server.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.mysql.host = '...'\nconnection_config.mysql.port = '...'\nconnection_config.mysql.database = '...'\nconnection_config.mysql.user = '...'\nconnection_config.mysql.password = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <p>Similarly, when using a MySQL instance with Google CloudSQL (quickstart, connect-overview), one could also use SSL option if applicable.</p> <pre><code>connection_config.mysql.ssl_options.key = '...'\nconnection_config.mysql.ssl_options.cert = '...'\nconnection_config.mysql.ssl_options.ca = '...'\nconnection_config.mysql.ssl_options.capath = '...'\nconnection_config.mysql.ssl_options.cipher = '...'\nconnection_config.mysql.ssl_options.verify_server_cert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>PostgreSQL connects to a PostgreSQL server.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.postgresql.host = '...'\nconnection_config.postgresql.port = '...'\nconnection_config.postgresql.user = '...'\nconnection_config.postgresql.password = '...'\nconnection_config.postgresql.dbname = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <p>Similarly, when using a PostgreSQL instance with Google CloudSQL (quickstart, connect-overview), one could also use SSL option if applicable.</p> <pre><code>connection_config.postgresql.ssloption.sslmode = '...' # disable, allow, verify-ca, verify-full, etc.\nconnection_config.postgresql.ssloption.sslcert = '...'\nconnection_config.postgresql.ssloption.sslkey = '...'\nconnection_config.postgresql.ssloption.sslpassword = '...'\nconnection_config.postgresql.ssloption.sslrootcert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre>"},{"location":"#data-model","title":"Data model","text":"<p>The Metadata Store uses the following data model to record and retrieve metadata from the storage backend.</p> <ul> <li><code>ArtifactType</code> describes an artifact's type and its properties that are     stored in the metadata store. You can register these types on-the-fly with     the metadata store in code, or you can load them in the store from a     serialized format. Once you register a type, its definition is available     throughout the lifetime of the store.</li> <li>An <code>Artifact</code> describes a specific instance of an <code>ArtifactType</code>, and its     properties that are written to the metadata store.</li> <li>An <code>ExecutionType</code> describes a type of component or step in a workflow, and     its runtime parameters.</li> <li>An <code>Execution</code> is a record of a component run or a step in an ML workflow     and the runtime parameters. An execution can be thought of as an instance of     an <code>ExecutionType</code>. Executions are recorded when you run an ML pipeline or     step.</li> <li>An <code>Event</code> is a record of the relationship between artifacts and executions.     When an execution happens, events record every artifact that was used by the     execution, and every artifact that was produced. These records allow for     lineage tracking throughout a workflow. By looking at all events, MLMD knows     what executions happened and what artifacts were created as a result. MLMD     can then recurse back from any artifact to all of its upstream inputs.</li> <li>A <code>ContextType</code> describes a type of conceptual group of artifacts and     executions in a workflow, and its structural properties. For example:     projects, pipeline runs, experiments, owners etc.</li> <li>A <code>Context</code> is an instance of a <code>ContextType</code>. It captures the shared     information within the group. For example: project name, changelist commit     id, experiment annotations etc. It has a user-defined unique name within its     <code>ContextType</code>.</li> <li>An <code>Attribution</code> is a record of the relationship between artifacts and     contexts.</li> <li>An <code>Association</code> is a record of the relationship between executions and     contexts.</li> </ul>"},{"location":"#mlmd-functionality","title":"MLMD Functionality","text":"<p>Tracking the inputs and outputs of all components/steps in an ML workflow and their lineage allows ML platforms to enable several important features. The following list provides a non-exhaustive overview of some of the major benefits.</p> <ul> <li>List all Artifacts of a specific type. Example: all Models that have     been trained.</li> <li>Load two Artifacts of the same type for comparison. Example: compare     results from two experiments.</li> <li>Show a DAG of all related executions and their input and output artifacts     of a context. Example: visualize the workflow of an experiment for     debugging and discovery.</li> <li>Recurse back through all events to see how an artifact was created.     Examples: see what data went into a model; enforce data retention plans.</li> <li>Identify all artifacts that were created using a given artifact.     Examples: see all Models trained from a specific dataset; mark models based     upon bad data.</li> <li>Determine if an execution has been run on the same inputs before.     Example: determine whether a component/step has already completed the same     work and the previous output can just be reused.</li> <li>Record and query context of workflow runs. Examples: track the owner and     changelist used for a workflow run; group the lineage by experiments; manage     artifacts by projects.</li> <li>Declarative nodes filtering capabilities on properties and 1-hop     neighborhood nodes. Examples: look for artifacts of a type and under some     pipeline context; return typed artifacts where a given property\u2019s value is     within a range; find previous executions in a context with the same inputs.</li> </ul> <p>See the MLMD tutorial for an example that shows you how to use the MLMD API and the metadata store to retrieve lineage information.</p>"},{"location":"#integrate-ml-metadata-into-your-ml-workflows","title":"Integrate ML Metadata into your ML Workflows","text":"<p>If you are a platform developer interested in integrating MLMD into your system, use the example workflow below to use the low-level MLMD APIs to track the execution of a training task. You can also use higher-level Python APIs in notebook environments to record experiment metadata.</p> <p></p> <p>1) Register artifact types</p> <pre><code># Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\ndata_type_id = store.put_artifact_type(data_type)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\nmodel_type_id = store.put_artifact_type(model_type)\n\n# Query all registered Artifact types.\nartifact_types = store.get_artifact_types()\n</code></pre> <p>2) Register execution types for all steps in the ML workflow</p> <pre><code># Create an ExecutionType, e.g., Trainer\ntrainer_type = metadata_store_pb2.ExecutionType()\ntrainer_type.name = \"Trainer\"\ntrainer_type.properties[\"state\"] = metadata_store_pb2.STRING\ntrainer_type_id = store.put_execution_type(trainer_type)\n\n# Query a registered Execution type with the returned id\n[registered_type] = store.get_execution_types_by_id([trainer_type_id])\n</code></pre> <p>3) Create an artifact of DataSet ArtifactType</p> <pre><code># Create an input artifact of type DataSet\ndata_artifact = metadata_store_pb2.Artifact()\ndata_artifact.uri = 'path/to/data'\ndata_artifact.properties[\"day\"].int_value = 1\ndata_artifact.properties[\"split\"].string_value = 'train'\ndata_artifact.type_id = data_type_id\n[data_artifact_id] = store.put_artifacts([data_artifact])\n\n# Query all registered Artifacts\nartifacts = store.get_artifacts()\n\n# Plus, there are many ways to query the same Artifact\n[stored_data_artifact] = store.get_artifacts_by_id([data_artifact_id])\nartifacts_with_uri = store.get_artifacts_by_uri(data_artifact.uri)\nartifacts_with_conditions = store.get_artifacts(\n      list_options=mlmd.ListOptions(\n          filter_query='uri LIKE \"%/data\" AND properties.day.int_value &gt; 0'))\n</code></pre> <p>4) Create an execution of the Trainer run</p> <pre><code># Register the Execution of a Trainer run\ntrainer_run = metadata_store_pb2.Execution()\ntrainer_run.type_id = trainer_type_id\ntrainer_run.properties[\"state\"].string_value = \"RUNNING\"\n[run_id] = store.put_executions([trainer_run])\n\n# Query all registered Execution\nexecutions = store.get_executions_by_id([run_id])\n# Similarly, the same execution can be queried with conditions.\nexecutions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query='type = \"Trainer\" AND properties.state.string_value IS NOT NULL'))\n</code></pre> <p>5) Define the input event and read data</p> <pre><code># Define the input event\ninput_event = metadata_store_pb2.Event()\ninput_event.artifact_id = data_artifact_id\ninput_event.execution_id = run_id\ninput_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n\n# Record the input event in the metadata store\nstore.put_events([input_event])\n</code></pre> <p>6) Declare the output artifact</p> <pre><code># Declare the output artifact of type SavedModel\nmodel_artifact = metadata_store_pb2.Artifact()\nmodel_artifact.uri = 'path/to/model/file'\nmodel_artifact.properties[\"version\"].int_value = 1\nmodel_artifact.properties[\"name\"].string_value = 'MNIST-v1'\nmodel_artifact.type_id = model_type_id\n[model_artifact_id] = store.put_artifacts([model_artifact])\n</code></pre> <p>7) Record the output event</p> <pre><code># Declare the output event\noutput_event = metadata_store_pb2.Event()\noutput_event.artifact_id = model_artifact_id\noutput_event.execution_id = run_id\noutput_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n\n# Submit output event to the Metadata Store\nstore.put_events([output_event])\n</code></pre> <p>8) Mark the execution as completed</p> <pre><code>trainer_run.id = run_id\ntrainer_run.properties[\"state\"].string_value = \"COMPLETED\"\nstore.put_executions([trainer_run])\n</code></pre> <p>9) Group artifacts and executions under a context using attributions and assertions artifacts</p> <pre><code># Create a ContextType, e.g., Experiment with a note property\nexperiment_type = metadata_store_pb2.ContextType()\nexperiment_type.name = \"Experiment\"\nexperiment_type.properties[\"note\"] = metadata_store_pb2.STRING\nexperiment_type_id = store.put_context_type(experiment_type)\n\n# Group the model and the trainer run to an experiment.\nmy_experiment = metadata_store_pb2.Context()\nmy_experiment.type_id = experiment_type_id\n# Give the experiment a name\nmy_experiment.name = \"exp1\"\nmy_experiment.properties[\"note\"].string_value = \"My first experiment.\"\n[experiment_id] = store.put_contexts([my_experiment])\n\nattribution = metadata_store_pb2.Attribution()\nattribution.artifact_id = model_artifact_id\nattribution.context_id = experiment_id\n\nassociation = metadata_store_pb2.Association()\nassociation.execution_id = run_id\nassociation.context_id = experiment_id\n\nstore.put_attributions_and_associations([attribution], [association])\n\n# Query the Artifacts and Executions that are linked to the Context.\nexperiment_artifacts = store.get_artifacts_by_context(experiment_id)\nexperiment_executions = store.get_executions_by_context(experiment_id)\n\n# You can also use neighborhood queries to fetch these artifacts and executions\n# with conditions.\nexperiment_artifacts_with_conditions = store.get_artifacts(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.type = \"Experiment\" AND contexts_a.name = \"exp1\"')))\nexperiment_executions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.id = {}'.format(experiment_id))))\n</code></pre>"},{"location":"#use-mlmd-with-a-remote-grpc-server","title":"Use MLMD with a remote gRPC server","text":"<p>You can use MLMD with remote gRPC servers as shown below:</p> <ul> <li>Start a server</li> </ul> <pre><code>bazel run -c opt --define grpc_no_ares=true  //ml_metadata/metadata_store:metadata_store_server\n</code></pre> <p>By default, the server uses a fake in-memory db per request and does not persist the metadata across calls. It can also be configured with a MLMD <code>MetadataStoreServerConfig</code> to use SQLite files or MySQL instances. The config can be stored in a text protobuf file and passed to the binary with <code>--metadata_store_server_config_file=path_to_the_config_file</code>.</p> <p>An example <code>MetadataStoreServerConfig</code> file in text protobuf format:</p> <pre><code>connection_config {\n  sqlite {\n    filename_uri: '/tmp/test_db'\n    connection_mode: READWRITE_OPENCREATE\n  }\n}\n</code></pre> <ul> <li>Create the client stub and use it in Python</li> </ul> <pre><code>from grpc import insecure_channel\nfrom ml_metadata.proto import metadata_store_pb2\nfrom ml_metadata.proto import metadata_store_service_pb2\nfrom ml_metadata.proto import metadata_store_service_pb2_grpc\n\nchannel = insecure_channel('localhost:8080')\nstub = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n</code></pre> <ul> <li>Use MLMD with RPC calls</li> </ul> <pre><code># Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\n\nrequest = metadata_store_service_pb2.PutArtifactTypeRequest()\nrequest.all_fields_match = True\nrequest.artifact_type.CopyFrom(data_type)\nstub.PutArtifactType(request)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\n\nrequest.artifact_type.CopyFrom(model_type)\nstub.PutArtifactType(request)\n</code></pre>"},{"location":"#upgrade-the-mlmd-library","title":"Upgrade the MLMD library","text":"<p>When using a new MLMD release or your own build with an existing MLMD database, there may be changes to the database schema. Unless a breaking change is explicitly mentioned in the release note, all MLMD database schema changes are transparent for the MLMD API users. If there is a breaking change notice, then old databases can still be upgraded to use the new MLMD library.</p> <p>When the MLMD library connects to the database, it compares the expected schema version of the MLMD library (<code>library_version</code>) with the schema version (<code>db_version</code>) recorded in the given database. By default, MLMD will check the compatibility and raise errors when the versions are incompatible.</p> <ul> <li>If <code>library_version</code> is compatible with <code>db_version</code>, nothing happens.</li> <li> <p>If <code>library_version</code> is newer than <code>db_version</code>, and auto-migration is not     enabled, then MLMD raises a failed precondition error with the following     message:</p> <pre><code>MLMD database version $db_version is older than library version\n$library_version. Schema migration is disabled. Please upgrade the\ndatabase then use the library version; or switch to a older library\nversion to use the current database.\n</code></pre> </li> <li> <p>If <code>library_version</code> is older than <code>db_version</code>, by default MLMD library     returns errors to prevent any data loss. In this case, you should upgrade     the library version before using that database.</p> </li> </ul>"},{"location":"#upgrade-the-database-schema","title":"Upgrade the database schema","text":"<p>MLMD provides utilities to upgrade the database version.</p> <p>For example, when connecting to a backend with a Python library:</p> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nstore = metadata_store.MetadataStore(connection_config,\n                                     enable_upgrade_migration=True)\n</code></pre> <p>Or when using gRPC server, set the MetadataStoreServerConfig as follows:</p> <pre><code>connection_config {\n    ...\n}\nmigration_options {\n    enable_upgrade_migration: true\n}\n</code></pre> <p>MLMD then evolves the database by executing a series of migration scripts. If the backend supports DDL queries within a transaction (e.g., SQLite), MLMD runs the steps together within a single transaction, and the transaction is rolled-back when an error occurs. The migration script is provided together with any schema-change commit and verified through testing.</p> <p>Note</p> <p>The migration DDLs in MySQL are not transactional. When using MySQL, there should only be a single connection with the upgrade migration enabled to use the old database. Take a backup of the database before upgrading to prevent potential data losses.</p>"},{"location":"#downgrade-the-database-schema","title":"Downgrade the database schema","text":"<p>A misconfiguration in the deployment of MLMD may cause an accidental upgrade, e.g., when you tries out a new version of the library and accidentally connect to the production instance of MLMD and upgrade the database. To recover from these situations, MLMD provides a downgrade feature. During connection, if the migration options specify the <code>downgrade_to_schema_version</code>, MLMD will run a downgrade transaction to revert the schema version and migrate the data, then terminate the connection. Once the downgrade is done, use the older version of the library to connect to the database.</p> <p>For example:</p> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nmetadata_store.downgrade_schema(connection_config,\n                                downgrade_to_schema_version = 0)\n</code></pre> <p>Note</p> <p>When downgrading, MLMD prevents data loss as much as possible. However, newer schema versions might be inherently more expressive and hence a downgrade can introduce data loss. When using backends that do not support DDL transactions (e.g., MySQL), the database should be backed up before downgrading and the downgrade script should be the only MLMD connection to the database.</p> <p>The list of <code>schema_version</code> used in MLMD releases are:</p> ml-metadata (MLMD) schema_version 1.16.0 10 1.15.0 10 1.14.0 10 1.13.1 10 1.13.0 10 1.12.0 10 1.11.0 10 1.10.0 8 1.9.0 8 1.8.0 8 1.7.0 8 1.6.0 7 1.5.0 7 1.4.0 7 1.3.0 7 1.2.0 7 1.1.0 7 1.0.0 6 0.30.0 6 0.29.0 6 0.28.0 6 0.27.0 6 0.26.0 6 0.25.1 6 0.24.0 5 0.23.0 5 0.22.1 5 0.21.2 4 0.15.2 4 0.14.0 4 0.13.2 0"},{"location":"#resources","title":"Resources","text":"<p>The MLMD library has a high-level API that you can readily use with your ML pipelines. See the MLMD API documentation for more details.</p> <p>Check out MLMD Declarative Nodes Filtering to learn how to use MLMD declarative nodes filtering capabilities on properties and 1-hop neighborhood nodes.</p> <p>Also check out the MLMD tutorial to learn how to use MLMD to trace the lineage of your pipeline components.</p>"}]}